{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "nWAeVCPg9WwL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cjpqpCe9WwN"
      },
      "source": [
        "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
        "There are several ways to find the coefficients of the regression: <br>\n",
        "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
        "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
        "$\\hspace{30mm}$ $F(x)$ = Loss Function = SE = $ \\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "G9WSq-ed9WwP"
      },
      "outputs": [],
      "source": [
        "class Linear_Regression():\n",
        "    def __init__(self, alpha = 1e-10 , num_iter = 10000, early_stop = 1e-50, intercept = True, init_weight = None):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "            Some initializations, if neccesary\n",
        "\n",
        "            attributes:\n",
        "                        alpha: Learning Rate, default 1e-10\n",
        "                        num_iter: Number of Iterations to update coefficient with training data\n",
        "                        early_stop: Constant control early_stop.\n",
        "                        intercept: Bool, If we are going to fit a intercept, default True.\n",
        "                        init_weight: Matrix (n x 1), input init_weight for testing.\n",
        "\n",
        "\n",
        "            TODO: 1. Initialize all variables needed.\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.num_iter = num_iter\n",
        "        self.early_stop = early_stop\n",
        "        self.intercept = intercept\n",
        "        self.init_weight = init_weight\n",
        "        self.model_name = 'Linear Regression'\n",
        "        self.coef = None\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "            Save the datasets in our model, and perform gradient descent.\n",
        "\n",
        "            Parameter:\n",
        "                X_train: Matrix or 2-D array. Input feature matrix.\n",
        "                Y_train: Matrix or 2-D array. Input target value.\n",
        "\n",
        "\n",
        "                TODO: 2. If we are going to fit the intercept, add a col with all 1's to the first column. (hint: np.hstack, np.ones)\n",
        "                      3. Initilaize our coef with uniform from [-1, 1] with the number of col in training set.\n",
        "                      4. Call the gradient_descent function to train.\n",
        "        \"\"\"\n",
        "\n",
        "        self.X = np.array(X_train)\n",
        "        self.y = np.array(y_train)\n",
        "\n",
        "        if self.intercept:\n",
        "            self.X = np.hstack((np.ones((self.X.shape[0], 1)), self.X))\n",
        "\n",
        "        num_cols = self.X.shape[1]\n",
        "        self.coef = np.random.uniform(-1, 1, size=(num_cols, 1))\n",
        "\n",
        "        self.gradient_descent()\n",
        "\n",
        "        return self\n",
        "    \n",
        "\n",
        "    def gradient(self):\n",
        "        \"\"\"\n",
        "            Helper function to calculate the gradient respect to coefficient.\n",
        "\n",
        "            TODO: 5. Think about the matrix format of the gradient of the loss function.\n",
        "        \"\"\"\n",
        "\n",
        "        y_pred = np.dot(self.X, self.coef)\n",
        "        error = y_pred - self.y\n",
        "        return np.dot(self.X.T, error) / len(self.y)\n",
        "\n",
        "    def gradient_descent(self):\n",
        "        \"\"\"\n",
        "        Training function\n",
        "\n",
        "        TODO:\n",
        "        6. Calculate the loss with current coefficients.\n",
        "        7. Update the temp_coef with learning rate and gradient.\n",
        "        8. Calculate the loss with temp_coef.\n",
        "        9. Implement the self-adaptive learning rate.\n",
        "            a. If current error is less than previous error, increase learning rate by a factor of 1.3.\n",
        "              And update coef with temp_coef.\n",
        "            b. If previous error is less than current error, decrease learning rate by a factor of 0.9.\n",
        "              Don't update coef.\n",
        "        10. Add the loss to the loss list we create.\n",
        "        \"\"\"\n",
        "\n",
        "        self.loss = []\n",
        "\n",
        "        for _ in range(self.num_iter):\n",
        "            \n",
        "            y_pred = np.dot(self.X, self.coef)\n",
        "            current_error = np.mean((y_pred - self.y) ** 2)\n",
        "            self.loss.append(current_error)\n",
        "            \n",
        "            gradient = self.gradient()\n",
        "            temp_coef = self.coef - self.alpha * gradient\n",
        "\n",
        "            y_pred_temp = np.dot(self.X, temp_coef)\n",
        "            temp_error = np.mean((y_pred_temp - self.y) ** 2)\n",
        "\n",
        "            if temp_error < self.early_stop or (self.loss and abs(temp_error - self.loss[-1]) < self.early_stop):\n",
        "                break\n",
        "\n",
        "            if temp_error < np.mean((np.dot(self.X, self.coef) - self.y) ** 2):\n",
        "                self.coef = temp_coef\n",
        "                self.alpha *= 1.3\n",
        "            else:\n",
        "                self.alpha *= 0.9\n",
        "\n",
        "            self.loss.append(temp_error)\n",
        "            \n",
        "            if abs(temp_error - current_error) < self.early_stop or abs(temp_error / current_error - 1) < self.early_stop:\n",
        "                break\n",
        "\n",
        "    def ind_predict(self, x: list):\n",
        "        \"\"\"\n",
        "        Predict the value based on its feature vector x.\n",
        "\n",
        "        Parameter:\n",
        "            x: Matrix, array or list. Input feature point.\n",
        "\n",
        "        Return:\n",
        "            result: prediction of given data point\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "        11. Implement the prediction function\n",
        "        \"\"\"\n",
        "\n",
        "        x = np.array(x)\n",
        "        prediction = np.dot(x, self.coef)\n",
        "        return prediction.item()\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "\n",
        "        if self.intercept:\n",
        "            X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "        predictions = np.dot(X, self.coef)\n",
        "        return predictions.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "Uosj4GL79WwR"
      },
      "outputs": [],
      "source": [
        "def min_max_normaliz(lst):\n",
        "    \"\"\"\n",
        "    Helper function for normalize for faster training.\n",
        "    \"\"\"\n",
        "    maximum = np.max(lst)\n",
        "    minimum = np.min(lst)\n",
        "\n",
        "    return (lst - minimum) / (maximum - minimum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ToNNxcg9WwR"
      },
      "source": [
        "### We generate some easy data for testing. We should fit a line with, $Y = 30 * X + 20$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "G-2mCwhD9WwS"
      },
      "outputs": [],
      "source": [
        "X = np.array(np.mat(np.arange(1, 1000, 5)).T)\n",
        "y = np.array((30 * X)).flatten() +  20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXEzyBpo9WwS"
      },
      "source": [
        "#### Do NOT modify the following line, just run it when you are done.  You can also try different initialization, you will notice different coef at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-a-Dmhc9WwS",
        "outputId": "69c81d21-b74b-4805-fb57-6ac66b1acddf",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.Linear_Regression at 0x29d460139d0>"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = Linear_Regression(alpha = 1, num_iter = 1000)\n",
        "clf.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7xA7mX69WwT"
      },
      "source": [
        "####  As the number of iteration increase, you should notice the coeficient converges to [20, 30].\n",
        "#### It maybe very slow update. Feel free to stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0hL92d_9WwU",
        "outputId": "6d93f765-28d5-4703-d4bc-55aa4e43d784",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([9.68219144e-01, 1.25036346e+00, 1.53250778e+00, ...,\n",
              "       4.41864550e+04, 4.44103701e+04, 4.46342851e+04])"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EmadgnM9WwU"
      },
      "source": [
        "#### Please try to normalize the X and fit again with normalized X. You should find something interesting. Also think about what you should do for predicting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe2igMJC9WwV",
        "outputId": "9e34de68-45d8-45f7-881b-486e96c538fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions (normalized): [   50.   200.   350. ... 29600. 29750. 29900.]\n"
          ]
        }
      ],
      "source": [
        "# Calculate mean and standard deviation of the feature matrix X\n",
        "X_mean = np.mean(X, axis=0)\n",
        "X_std = np.std(X, axis=0)\n",
        "\n",
        "# Normalize the feature matrix X\n",
        "X_normalized = (X - X_mean) / X_std\n",
        "\n",
        "# Initialize a Linear Regression model with specified hyperparameters\n",
        "clf_normalized = Linear_Regression(alpha=1, num_iter=1000)\n",
        "\n",
        "# Fit the model to the normalized feature matrix X_normalized and target values y\n",
        "clf_normalized.fit(X_normalized, y)\n",
        "\n",
        "# Normalize the test data using the same mean and standard deviation as the training data\n",
        "X_test_normalized = (X - X_mean) / X_std\n",
        "\n",
        "# Obtain predictions from the fitted model using the normalized test data\n",
        "predictions_normalized = clf_normalized.predict(X_test_normalized)\n",
        "\n",
        "# Print the predictions obtained from the model trained on normalized data\n",
        "print(\"Predictions (normalized):\", predictions_normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMKR47Te9WwV"
      },
      "source": [
        "##### You can also try this with the wine dataset we use in HW1. Try fit this function to that dataset with same features. If you look closely to the updates of coefficients. What do you find? This could be mentioned in your report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "BX7q--GT9WwV"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "7IS_kBGJ9WwV"
      },
      "outputs": [],
      "source": [
        "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "wine = pd.read_csv(url_Wine, delimiter=';')\n",
        "X = wine[['density','alcohol']]\n",
        "y = wine.quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfUGVDav9WwV",
        "outputId": "d6f78cd1-eab5-4559-a2d1-495eda0803a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "800.6676988774337"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(X,y)\n",
        "## Squared Error with sklearn.\n",
        "sum((lr.predict(X) - y)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRI648bI9WwW"
      },
      "source": [
        "#### You will notice different coefficients, but the loss is very close to each other like 805. In your report, briefly discuss this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1h6MSIL9WwW",
        "outputId": "eb519c4d-2718-4f2e-f95a-6d8d83df01a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.Linear_Regression at 0x29d4042aa50>"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = Linear_Regression(alpha = 1, num_iter = 1)\n",
        "clf.fit(X,y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsBBEo-89WwW",
        "outputId": "6d585358-8508-4d31-db05-5cc451e5989b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "247899.12638757518"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum((clf.predict(X) - y)**2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
